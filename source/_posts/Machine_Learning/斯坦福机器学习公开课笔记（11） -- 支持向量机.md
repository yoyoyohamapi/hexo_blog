title: 斯坦福机器学习公开课笔记（11）-- 支持向量机
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----

## 最优化目标（Optimization objective）
先回顾__逻辑回归（logic regression）__中的假设函数：

{% math_block %}
h_\theta(x) = \frac{1}{1+e^{-\theta x}}
{% endmath_block %}

此时，代价函数：

{% math_block %}
cost=-[ylogh_\theta(x)+(1-y)log(1-h_\theta(x))]
{% endmath_block %}

如果 {% math y=1 %}，显然，此时我们想要 {% math \theta^Tx \gg 0 %}，此时预测函数更加接近于 {% math 1 %}。

此时，代价函数：

{% math_block %}
cost=-log\frac{1}{1+e^{-z}},(z=\theta x)
{% endmath_block %}

观察到代价函数随 {% math z %}的变化曲线：

![y=1,变化曲线](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_cost1.jpg)

观察这条曲线发现，当 {% math z %} 很大时，代价函数 $-log\frac{1}{1+e^{-z}}$ 的取值很小，即预测代价很小。这也就解释了为什么逻辑回归在观察到正样本 {% math y=1 %}时，会把 {% math z=\theta x %}设置得很大，因为此时能使预测代价尽可能小。

接下来，我们改动一下变化曲线，将曲线拉直为折线，构成了SVM中$y=1$时的预测代价 {% math cost_1(z) %}, 这里的下标 {% math 1 %}即代表 {% math  y=1 %} 。同理，{% math y=0 %} 时的代价为 {% math cost_0(z) %}。

![y=0,变化曲线](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_cost0.jpg)

据此，我们可由最小化逻辑回归代价函数的过程推导出最小化SVM代价函数的过程。

__Logic Regression__:

{% math_block %}
\min\limits_{\theta}\frac{1}{m}[\sum\limits_{i=1}^{m}y^{(i)}(-logh_\theta(x^{(i)}))+(1-y^{(i)})(-log(1-h_\theta(x^{(i)})))] + \frac{1}{2m}\sum\limits_{j=1}^{n}\theta_j^2
{% endmath_block %}


__SVM__:

{% math_block %}
\min\limits_{\theta}C[\sum\limits_{i=1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2
{% endmath_block %}

> 在逻辑回归中，代价函数可以概括为两部分：

> {% math_block %}
cost=A+\lambda B
{% endmath_block %}

> 正规化参数 {% math \lambda %}用来调节 {% math A,B %}在代价函数中所占的比重，{% math \lambda %}越大，{% math A %}占比越小。

> 在SVM中，代价函数可以概括为：

> {% math_block %}
cost=CA+B
{% endmath_block %}

> {% math C %}越小，A占比越小，亦即 {% math C %}扮演了 {% math \frac{1}{\lambda} %}的角色。

 SVM的预测函数：

{% math_block %}
h_\theta(x)=\begin{cases} 1 &\mbox{if $\theta^Tx \gg 0$} \\ 0 &\mbox{otherwise} \end{cases}
{% endmath_block %}

## 大间距分类器（Large Margin Classifiers）

![SVM中更加严苛的限制](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_svm_restrict.jpg)

如图，在SVM中，相比于逻辑回归，如果 {% math y=1 %}，我们不只希望 {% math \theta^T x \geq 0 %}，更希望 {% math \theta^T x \geq 1 %}。

假如，我们把参数 {% math C %}设置得非常大时，显然，在最小化 {% math \theta %}的过程中，我们就希望上节提到的 {% math A %}项尽可能小，最好是小到0。为使其小到0，就有：

当 {% math y=1 %}时：

{% math_block %}
\theta^T x \geq 1
{% endmath_block %}

当 {% math y=0 %}时：

{% math_block %}
\theta^T x \leq -1
{% endmath_block %}

此时，我们最小化的问题就化简为：

{% math_block %}
\min\limits_\theta \frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2
{% endmath_block %}

限制条件为：

{% math_block %}
\begin{cases} \theta^T x \geq 1 &\mbox{if $y=1$} \\ \theta^T x \leq -1 &\mbox{if $y=0$} \end{cases}
{% endmath_block %}

那么，在执行这个最小化的过程中，就会发现如下图所示的有趣现象：

![large margin](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_linear_margin.jpg)

其中，黑色的决策线为最终决策线，相比较于绿色及紫色分界线，其距离两种分类的距离足够远，这就是所谓的__大间距（Largin Margin）__-- 不同类型与决策线的距离尽可能远。

>当参数 {% math C %}非常大时，最小化代价函数时就会出现有趣的大间距决策边界，而当参数 {% math C %}较小时，决策边界不再会对新出现的异常样本变得敏感(决策线不会剧烈摆动)。

![C的取值对于决策线的影响](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_C_margin.jpg)

至于为什么在 {% math C %}设置较大时会出现这样的现象，下节会给出答案。

## 大间距来由

对于如下两个向量：

{% math_block %}
u=\left(\begin{matrix}u_1 \\ u_2\end{matrix}\right),v=\left(\begin{matrix}v_1 \\ v_2\end{matrix}\right)
{% endmath_block %}

我们可以推导其向量__内积__的计算式：

{% math_block %}
\begin{aligned}u^Tv &= p \cdot ||u|| \\& = u_1v_1+u_2v_2 \end{aligned}
{% endmath_block %}

其中，{% math p %}为向量 {% math v %}投影到向量 {% math u %}的长度，可正可负，故而向量内积也可正可负。

考虑上一节中的最小化问题：

{% math_block %}
\min\limits_\theta \frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2
{% endmath_block %}

限制条件为：

{% math_block %}
\begin{cases} \theta^T x^{(i)} \geq 1 &\mbox{if $y^{(i)}=1$} \\ \theta^T x^{(i)} \leq -1 &\mbox{if $y^{(i)}=0$} \end{cases}
{% endmath_block %}

假设我们的 {% math \theta=\left(\begin{matrix}\theta_1 \\ \theta_2\end{matrix}\right)$,$\theta_0=0 \mbox{ (使得$\theta$过原点)} %} ，那么：

{% math_block %}
\min\limits_\theta\frac{1}{2}\sum\limits_{j=1}^{2}\theta_j^2=\min\limits_\theta\frac{1}{2}(\theta_1+\theta_2)^2=\min\limits_\theta\frac{1}{2}(\sqrt{\theta_1^2+\theta_2^2})^2=\frac{1}{2}||\theta||^2
{% endmath_block %}

由向量内积公式：

{% math_block %}
\theta^Tx^{(i)}=p^{(i)} \cdot ||\theta||
{% endmath_block %}

其中，{% math p^{(i)} %} 为 {% math x^{(i)} %}在 {% math \theta %} 上的投影， 如下图所示：

![](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_projection.jpg)

当 {% math p^{(i)} %}非常小时，为使 {% math p^{(i)} \cdot ||\theta|| \geq 1 %}，显然就必须使得 {% math ||\theta|| %}非常大。

反之，若 {% math p^(i) %}足够长（实际上也就是形成了大间距），就可使范数 {% math ||\theta|| %}的要求变小，{% math ||\theta|| %}变小，则 {% math \frac{1}{2}||\theta||^2 %}变小，也就达到了最小化代价函数的目的, 这也就是__为什么SVM会具有大间距分类的性质（大间距带来小范数）__。

![大间距形成](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_p_large_margin.jpg)


## 核函数（kernel）

### 什么是核函数
假定我们有如下预测：

predict {% math y %} if :

{% math_block %}
\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots \geq 0
{% endmath_block %}

引入新的预测定义：

{% math_block %}
\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots \geq 0
{% endmath_block %}

其中：

{% math_block %}
f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,f_5=x_2^2
{% endmath_block %}

但我们并不知道这些高阶项是否有用，如何更有意义的选择特征，构建预测函数呢？

在SVM中，我们会选取一些__标记点（landmark）__，并且通过特征 {% math f_i %}来衡量 {% math x %}与 {% math l_i %}的相似程度（距离） ，如：

{% math_block %}
f_1=similarity(x,l_i)=exp(-\frac{||x-l^{(1)}||^2}{2\delta^2})
{% endmath_block %}

![landmark](http://7pulhb.com1.z0.glb.clouddn.com/ML-w7_landmark.jpg)

这个距离度量在SVM中就称之为__核函数（kernel）__，上面这个核函数为所用最广的__高斯核函数（Gaussian Kernel）__。

并且注意到：

 {% math \mbox{If } x \approx l^{(i)} \mbox{距离接近时：} %}

{% math_block %}
f_1 \approx exp(-\frac{0^2}{2\delta^2}) \approx 1
{% endmath_block %}

 {% math \mbox{If } x \mbox{ 远离 } l^{(i)}: %}

{% math_block %}
f_1 \approx exp(-\frac{(\mbox{large number})^2}{2\delta^2}) \approx 0
{% endmath_block %}

### 如何选取标记点（landmark）
在SVM中，往往把样本点选作标记点，这样做的好处就是使得核函数描述的是__每个样本点与其他样本点的距离__。

假定我们给定了一组样本：

{% math_block %}
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})，(x^{(3)},y^{(3)}) \cdots (x^{(m)},y^{(m)})
{% endmath_block %}

选取每个样本点作为标记点：

{% math_block %}
l^{(1)}=x^{(1)}，l^{(2)}=x^{(2)}，l^{(3)}=x^{(3)} \cdots l^{(m)}=x^{(m)}
{% endmath_block %}

对于训练样本： {% math (x^{(i)},y^{(i)}) %}

{% math_block %}
f_1{(i)}=sim(x^{(i)},l^{(i)})
{% endmath_block %}

{% math_block %}
f_2{(i)}=sim(x^{(i)},l^{(2)})
{% endmath_block %}

{% math_block %}
\vdots
{% endmath_block %}

{% math_block %}
f_m{(i)}=sim(x^{(i)},l^{(m)})
{% endmath_block %}

新的特征向量：

{% math_block %}
f=\left(\begin{matrix}f_0 \\ f_1 \\ f_2 \\ \vdots f_m  \end{matrix}\right),f_0=1
{% endmath_block %}

由此，我们得到SVM的训练过程如下：

{% math_block %}
\min\limits_{\theta}C[\sum\limits_{i=1}^{m}y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2
{% endmath_block %}

## 如何使用SVM

仅观看了机器学习课程而去尝试自己撰写SVM代码是不明智的，该课程仅介绍了SVM的大致原理，实际上，SVM的实现还涉及到很多细节问题，再者，作为现如今使用最广的机器学习算法，SVM已经有了诸如[libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/†)在内的一大批成熟优秀高性能的实现库，我们学会怎么使用这些库远比自己书写实现代码有意义。

但在使用这些库的时候，也需要声明SVM关键的两个部分：

1. 参数 {% math C %}
2. 核函数（Kernel）

其中，核函数的选择有如下策略：

* __无核函数（No kernel）__，即不在考量样本与标记点的相似程度，而用原来的预测方式：

{% math_block %}
\mbox{Predict  } y=1 \mbox{  if  } \theta^Tx \geq 0
{% endmath_block %}

该选择方式适用于以下情况：

{% math_block %}
\begin{cases} 特征维数n高 \\ 样本容量m小 \\ x \in R^{n+1} \end{cases}
{% endmath_block %}

如果在这种情况下仍然用了核函数，反而容易导致过拟合。

* __高斯核函数（Gaussian Kernel）__

{% math_block %}
f_i=exp(\frac{||x-l^{(i)}||^2}{2\delta^2}),\mbox{where } l^{i}=x^{(i)}
{% endmath_block %}

高斯核函数适用于:

{% math_block %}
\begin{cases} 特征维数n小 \\ 样本容量m大 \\ x \in R^{n} \end{cases}
{% endmath_block %}

> 在使用高斯核函数时，需要做特征归一化（feature scaling），以式SVM同等程度的关注到不同的特征（可能导致导致公示当中的范数部分差异过大）。

* __多项式核函数（Polynomial kernel）__

{% math_block %}
k=(x^T+C)^d
{% endmath_block %}

该核函数用于 {% math x %}及 {% math l %}都是非常严格的非负数时

> 在大部分的生产环境下，高斯核函数已经足够使用了。


## 逻辑回归于SVM的选择
由于二者都是用于有监督的分类问题，故而何时使用逻辑回归，何时使用SVM就成了必须面对的一个问题。通常有如下考量：

（1） 如果 {% math n %} 相对于 {% math m %} 很大的话（例如 {% math n=10,000 %}， {% math m=10-1000 %}）

选择逻辑回归或者无核SVM

（2）{% math n %} 很小， {% math m %} 适中。（例如，$n=1-1000 %}， {% math m=10-10000 %}）

选择使用了高斯核函数的SVM

（3）如果 {% math n %}很小而 {% math m %} 很大时，（如 {% math n=1-1000$， {% math m>50000 %}）

此时需要手动创建更多的特征，之后用逻辑回归或者无核的SVM