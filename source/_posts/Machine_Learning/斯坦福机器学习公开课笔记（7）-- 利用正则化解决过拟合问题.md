title: 斯坦福机器学习公开课笔记（7）-- 利用正则化解决过拟合问题
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----
在之前的文章中，我们认识了[过拟合问题](http://yoyoyohamapi.com/?p=546),通常，我们有如下策略来解决过拟合问题：

1. 减少特征数，显然这只是权宜之计，因为特征意味着信息，放弃特征也就等同于丢弃信息，恩，舍不得，要知道，特征的获取往往也是艰苦卓绝的。

2. 不放弃特征，而是拉伸曲线使之更加平滑以解决过拟合问题，为了拉伸曲线，也就要弱化一些高阶项（曲线曲折的罪魁祸首），由于高阶项中的特征无法更改也就无法弱化，我们能弱化的只有高阶项中的系数{% math \theta_i%}，并且，我们把这种弱化称之为是对参数 $\theta$ 的惩罚（penalize）。__Regularization(规则化)__正是完成这样一种惩罚的“侩子手”，如下例所示，我们将 $\theta_3 $及 $\theta_4$ 减小到接近0，原本过拟合的曲线就变得更加平滑，接近于一条二次曲线（在本例中，二次曲线显然更能反映住房面积和房价的关系），也就能够更好的根据住房面积来预测房价（要知道，预测才是我们的最终目的，而非拟合）。

![eg.](http://7pulhb.com1.z0.glb.clouddn.com/ml-regularization.png)

*****************************************

###线性回归（Linear Regression）问题中的Regularization

在线性回归中，我们的预测代价如下评估：

{% math_block %}
J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
{% endmath_block %}

为了在最小化$J(\theta)$的过程中，也能尽可能使$\theta$变小，我们将上式更改为:

{% math_block %}
J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum\limits_{i=1}^{n}\theta_j^2
{% endmath_block %}

其中，参数 $\lambda$ 主要是完成以下两个任务:

1. 保证对数据的拟合良好
2. 保证 $\theta$ 足够小，避免过拟合问题。

> $\lambda$ 越大，要使 $J(\theta)$ 变小，惩罚力度就要变大，这样 $\theta$ 会被惩罚得越惨（越小），即要避免过拟合，我们显然应当增大 $\lambda$ 的值。

那么，梯度下降也发生相应变化：

Repeat{

{% math_block %}
\theta_0=\theta_0-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_0^{(i)}
{% endmath_block %}

{% math_block %}
\theta_j=\theta_j-\alpha[\frac{1}{m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}+\frac{\lambda}{m}\theta_j] (1)
{% endmath_block %}

}

其中，__（1）__式等价于：

{% math_block %}
\theta_j=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}
{% endmath_block %}

由于$1-\alpha\frac{\lambda}{m}\lt1$,故而梯度下降中每次更新$\theta$，同时也会去减小$\theta$值，达到了Regularization的目的。

在Normal Equation中，使$J(\theta)$最小化的$\theta$值为：

{% math_block %}
\theta=(X^TX+\lambda\left[\begin{array}{ccccc}0 &\cdots &\cdots &\cdots &0 \\ 0 &1 &\cdots &\cdots &0\\ \vdots & \vdots & 1 &\cdots & 0\\ \vdots &\vdots &\cdots &\ddots & \vdots \\ 0 & 0 &\cdots &\cdots &1 \end{array}\right])^{-1}X^Ty
{% endmath_block %}

###逻辑回归（Logic Regression）问题中的Regularization

代价评估如下：

{% math_block %}
J(\theta)=-[\frac{1}{m}\sum\limits_{i=1}^{m}y^{(i)}logh_0(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta_j^2
{% endmath_block %}

梯度下降如下:

Repeat{
{% math_block %}
\theta_0=\theta_0-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_0^{(i)}
{% endmath_block %}

{% math_block %}
\theta_j=\theta_j-\alpha[\frac{1}{m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}+\frac{\lambda}{m}\theta_j] (1)
{% endmath_block %}

}

> 注意，形式虽然与线性回归中的一样，但是注意预测函数$h_\theta(x)$二者是不同的。

***********************************************
在试验中，分别测试了三个$\lambda$值，分别是0,1,100，0即意味着不做Regularization，因为0值时候已经拟合得足够好了，所以当取1值时，对拟合曲线的光滑效果也不甚明显，但是当$\lambda$取100时，就造成了欠拟合效果，因为此时$\theta$已经偏离正规拟合太多：

![0](http://7pulhb.com1.z0.glb.clouddn.com/ml-lambda=0.png)

![1](http://7pulhb.com1.z0.glb.clouddn.com/ml-lambda=1.png)

![100](http://7pulhb.com1.z0.glb.clouddn.com/ml-lambda=100.png)

多项式分类的实现代码如下：

```python
#coding:utf-8
'''
    - Created with Sublime Text 2.
    - User: yoyoyohamapi吴晓军
    - Date: 2015-01-09
    - Time: 10:54:04
    - Contact: yoyoyohamapi.com
'''

###################################
#      实现Logic Regression       #
#      科学计算通过numpy完成      #
#      图像绘制通过matplotlib完成 #
###################################
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pylab as plb
from sympy import *
"""
定义sigmoid函数
"""
def sigmoid(z):
	return 1.0/(1.0+np.exp(-z))

"""
定义代价函数
	:param x:输入矩阵
	:param y:分类向量
	:param theta:拟合参数theta
	:return:预测精度
"""
def cost(x,y,theta):
	z = np.dot(x,theta)
	return -y*np.log(sigmoid(z)) 
	- (1-y)*np.log(1 - sigmoid(z))

""" 
定义代价函数评估
	:param x:输入矩阵
	:param y:分类向量
	:param theta:拟合参数theta
	:return:预测精度
"""
def J(x,y,theta,theLambda):
	return np.mean( cost(x,y,theta) )+theLambda*np.mean(theta)/2

"""
定义逻辑回归训练函数
	:param train_set_x:训练集--特征
	:param train_set_y:训练集--分类
	:param theta:初始化参数theta
	:param options:训练选项:
				   1.alpha 学习率
				   2.max_loop 最大迭代次数
				   3.eplison 收敛精度
				   4.debug 是否观察预测精度J的变化
				   5.method 训练方法:BGD？SGD？
	:return :拟合参数
"""

def trainLogicRegression(train_set_x,train_set_y,theta,options):
	# m:样本数
	# n:特征数
	m,n = train_set_x.shape
	
	# 迭代次数
	count = 0

	# 训练参数初始化
	alpha = options['alpha']
	max_loop = options['max_loop']
	eplison = options['eplison']
	debug = options['debug']
	method = options['method']
	theLambda = options['theLambda']

	methods = {
		'BGDdebug':BGDdebug,
		'SGDdebug':SGDdebug,
		'BGD':BGD,
		'SGD':SGD,
	}

	# 保存上次参数theta
	last = theta.copy()
	if debug:
		theta,count= methods.get(method+'debug')(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m,theLambda)
	else:
		theta,count= methods.get(method)(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m,theLambda)
	print "The number of iteration is :%d"%count
	return theta,count

"""
定义Batch Gradient Descent
"""
def BGDdebug(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m,theLambda):
	plt.figure()
	while count <= max_loop:
		count=count+1
		diff = sigmoid(np.dot(train_set_x,theta)) - train_set_y
		theta = theta - alpha*(np.dot(train_set_x.T,diff)/m+theLambda*theta/m)
		# 绘制预测精度状况
		plt.scatter(
			count,
			J(train_set_x,train_set_y,theta,theLambda),
			marker='o',
			color='y',
			s=50)

		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	plt.show()
	return theta,count

def BGD(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m,theLambda):
	while count <= max_loop:
		count=count+1
		diff = sigmoid(np.dot(train_set_x,theta)) - train_set_y
		theta = theta - alpha*(np.dot(train_set_x.T,diff)/m+theLambda*theta/m)
		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	return theta,count

"""
定义Stotochastic Gradient Descent
"""
def SGDdebug(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m,theLambda):
	plt.figure()
	while count <= max_loop:
		count=count+1
		for i in range(m):
			diff = sigmoid(np.dot(train_set_x[i],theta)) - train_set_y[i]
			theta = (theta.T - alpha*(train_set_x[i]-theLambda*theta.T)*diff).T
		# 绘制预测精度状况
		plt.scatter(
			count,
			J(train_set_x,train_set_y,theta,theLambda),
			marker='o',
			color='y',
			s=50)

		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	plt.xlabel('Iteration Step')
	plt.ylabel('Cost Evaluation')
	plt.show()
	return theta,count

def SGD(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m,theLambda):
	while count <= max_loop:
		count=count+1
		for i in range(m):
			diff = sigmoid(np.dot(train_set_x[i],theta)) - train_set_y[i]
			theta = (theta.T - alpha*(train_set_x[i]-theLambda*theta.T)*diff).T
		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	return theta,count

"""
定义结果绘制函数
	:param train_set_x:训练集--特征
	:param train_set_y:训练集--分类
	:param theta:参数向量
"""
def showTrainResult(train_set_x,train_set_y,theta,theLambda):
	# m:样本数
	# n:特征数
	m,n = train_set_x.shape

	x1 = np.linspace(np.min(train_set_x[0:m,1:2]),np.max(train_set_x[0:m,1:2]),256)
	x2 = np.linspace(np.min(train_set_x[0:m,2:3]),np.max(train_set_x[0:m,2:3]),256)
	x1,x2 = np.meshgrid(x1,x2)

	plt.figure()
	# 绘制样本点
	for i in range(m):
		if train_set_y[i]==0:
			plt.scatter(train_set_x[i,1],train_set_x[i,2],marker='o',color='y',s=50)
		else:
			plt.scatter(train_set_x[i,1],train_set_x[i,2],marker='*',color='r',s=50)
	# 绘制决策边界
	cs = plt.contour(x1, x2, f(x1,x2,theta), 1, colors='green', linewidth=.5)
	# 添加图示
	plt.scatter(train_set_x[0,1],train_set_x[0,2],marker='o',color='y',s=50,label="y=0")
	plt.scatter(train_set_x[0,1],train_set_x[0,2],marker='*',color='r',s=50,label="y=1")
	cs.collections[0].set_label("Decision Boundry")
	plt.title('lambda=%d'%theLambda)
	plt.legend(loc='upper left')
	plt.xlabel('X1')
	plt.ylabel('X2')
	plt.show()		


"""
定义决策边界多项式
"""
def f(x1,x2,theta):
	z = 0
	degree = 6
	index = 1
	for i in range(1,degree+1):
		for j in range(i+1):
			z = z+theta[index]*pow(x1,i-j)*pow(x2,j)
			index = index+1
	return z+theta[0]
```
 
测试代码如下：

```python
# coding:utf-8
'''
    - Created with Sublime Text 2.
    - User: yoyoyohamapi
    - Date: 2015-01-13
    - Time: 09:15:22
    - Contact: yoyoyohamapi.com
'''

###################################
#                                 #
#      测试Logic Regression       #
#                                 #
###################################

import numpy as np
import time
from logic_regression import *

# 载入数据
b = np.loadtxt("data2.txt")

# 特征集,28维
train_set_x = np.ones([b.shape[0],28],dtype=float)

# m:样本数
# n:特征数
m,n = train_set_x.shape

# 初始化训练样本,28维
train_set_x[0:m,1:2] = b[0:m,0:1] #x1
train_set_x[0:m,2:3] = b[0:m,1:2] #x2
x1 = train_set_x[0:m,1:2].copy()
x2 = train_set_x[0:m,2:3].copy()

degree = 6
index = 0
for i in range(1,degree+1):
	for j in range(i+1):
		index = index + 1
		train_set_x[0:m,index:index+1] = pow(x1,i-j)*pow(x2,j)



# 分类向量
train_set_y = b[0:m,2:3]

# 初始化theta
theta = np.zeros([n,1],dtype=float)

# 迭代次数
count = 0

# 定义训练参数
options = {
	'alpha':0.9,
	'max_loop':5000,
	'eplison':0.001,
	'debug':True,
	'theLambda':1,
	'method':'BGD'
}

#########################
#--------训练开始-------#
#########################

print 'Trainning start...................'
# 计时
start = time.clock()
# 训练 
theta,count = trainLogicRegression(train_set_x,train_set_y,theta,options)
# 停止计时
end = time.clock()
time_cost = end - start

#########################
#--------显示结果-------#
#########################
print 'Trainning end,the number of iteration is:%d'%count
print 'Time consumed:%ld'%time_cost+'s'
print 'Vector theta is:'
print theta
showTrainResult(train_set_x,train_set_y,theta,options['theLambda'])
```