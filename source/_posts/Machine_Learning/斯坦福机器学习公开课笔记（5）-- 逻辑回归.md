title: 斯坦福机器学习公开课笔记（5）-- 逻辑回归
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----
###为什么不用线性回归(linear regression)来做_0-1_分类?

假定我们定义阈值函数为：

$h_\theta(x)\gt0.5 => y=1$

$h_\theta(x)\leq0.5 => y=0$

图1:

![before](http://7pulhb.com1.z0.glb.clouddn.com/ml-before.png)

图2:

![after](http://7pulhb.com1.z0.glb.clouddn.com/ml-after.png)

上图都是进行的线性预测，在图1中，拟合曲线大致地分了类，但是，如图2所示，如果我们新增了一个输入，此时拟合曲线发生变化，由紫色线旋转到蓝色线，导致新增加的“1”类被视作了“0”类。

并且，传统的线性回归预测函数会有这样的问题:{% math h_\theta(x)>1 %}或者 {% math h_\theta(x)<0 %} ,这和我们只存在“0”，“1”分类是相矛盾的。

基于如上的原因，我们无法用线性回归来完成二分类问题，而会用接下来谈到的逻辑回归进行此事，逻辑一词实际上也是指代__是（1）__与__非（0）__。
******************************************

###逻辑回归（Logic Regression）的预测函数
在逻辑回归中，我们定义预测函数为：

{% math_block %}
h_\theta(x)=g(z) 
{% endmath_block %}
其中,$z=\theta^Tx$是__分类边界__，且$g(z)=\frac{1}{1+e^{-z}}$


该函数称之为Sigmoid Function，依称Logic Function（逻辑一词，即是衡量是(1)与非(0)），函数图像如下：

![sigmoid](http://7pulhb.com1.z0.glb.clouddn.com/ml-sigmoid.jpg)

可以看到，$h_\theta(x)$被很好地限制在了0，1之间，并且，sigmoid是一个非常好的阈值（阈值为0.5，0.5以上为“1”类，0.5一下为“0”类）函数：过渡光滑自然，关于0.5中心对称也极具美感。

********************************************
###决策边界（Decision Boundry）
这个命名我是极喜欢的，Boundry指边界，边界就可能有很多种形态，可能是线，可能是面。但是边界总是目的明确地，就是划清界限，故而，Andrew Ng强调“决策边界是{% math h_\theta(x) %}的属性，而非训练集training set的属性”，因为能够产生划清界限这一行为的是{% math h_\theta(x) %}(在Sigmoid中，以0.5为界)，而训练集仅只是用来训练参数{% math \theta %}的。

下面两幅图分别展示了线性决策边界和非线性决策边界：

线性决策边界：

![linear decision boundry](http://7pulhb.com1.z0.glb.clouddn.com/ml-linear_boundry.png)

非线性决策边界:

![non-linear decision boundry](http://7pulhb.com1.z0.glb.clouddn.com/ml-Non-linearBoundry.png)

对于分类任务，我们就是要反复调节$\theta$,以使得我们能够做出更精确的预测。从图像上来说，就是不断转动我们的决策边界来区分0类和1类。
************************************************************

###如何选择参数$\theta$?

我们要选择参数$\theta$使得预测的代价足够小，即做出的预测足够接近真实值，要注意到，对于代价函数$J(\theta)$，越少的极小值就越容易保证我们能找到最小值，亦即越容易找到我们代价花费最少的位置。

下图左图这样犬牙差互的代价曲线（非凸函数）显然会使我们在做梯度下降的时候陷入迷茫，任何一个极小值都有可能被判断为最小值，然而他可能是“代价高昂的”，

![non-convex function](http://7pulhb.com1.z0.glb.clouddn.com/ml-convex.png)

但是，如果保证了$J(\theta)$是凸函数，如上图右图所示，那么我们就能保证梯度下降时能够收敛到最小值。

关于$J(\theta)$,我们是这样定义的：

{% math_block %}
J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost[h_\theta(x^{(i)}),y^{(i)}]
{% endmath_block %}

为保证$J(\theta)$是凸函数，在逻辑回归中，我们这样定义$Cost(h_\theta(x),y)$:

{% math_block %}
Cost(h_\theta(x),y)=\begin{cases}-log(h_\theta(x))&\mbox{if $y=1$}\\-log(1-h_\theta(x))&\mbox{if $y=0$}\end{cases}
{% endmath_block %}

该函数等价于：

{% math_block %}
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
{% endmath_block %}

函数图像如下：

![cost function](http://7pulhb.com1.z0.glb.clouddn.com/ml-LR_cost.png)

![cost function2](http://7pulhb.com1.z0.glb.clouddn.com/ml-LR_cost2.png)

可以看到，如果预测正确，即$h_\theta(x)=y$,则$cost=0$，完美预测。并且cost随着预测偏差呈指数型变化。

*********************************

###利用梯度下降（Gradient Descent）来最小化$J(\theta)$

若是批量梯度下降（Batch Gradient Descent），则有如下过程:

Repeat Until Convergence{

{% math_block %}
\theta_j=\theta_j-\alpha\sum\limits_{i=1}^{m}[(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}]
{% endmath_block %}

}

以上式子中，$m$为训练样本数,$\alpha$为学习率，$h_\theta(x^{(i)})=\frac{1}{1+e^{-\theta^Tx^{(i)}}}$

若是随机梯度下降（Stotochastic Gradient Descent），则有如下过程：

Loop {

for i in m {

{% math_block %}
\theta_j = \theta_j-\alpha*[(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}]
{% endmath_block %}

}

}

*******************************
下面是Logic Regression的代码实现，目前只实现了两个特征的逻辑回归:

```python
#coding:utf-8
'''
    - Created with Sublime Text 2.
    - User: yoyoyohamapi吴晓军
    - Date: 2015-01-09
    - Time: 10:54:04
    - Contact: yoyoyohamapi.com
'''

###################################
#      实现Logic Regression       #
#      科学计算通过numpy完成      #
#      图像绘制通过matplotlib完成 #
###################################
import numpy as np
import matplotlib.pyplot as plt
"""
定义sigmoid函数
"""
def sigmoid(z):
	return 1.0/(1.0+np.exp(-z))

"""
定义代价函数
	:param x:输入矩阵
	:param y:分类向量
	:param theta:拟合参数theta
	:return:预测精度
"""
def cost(x,y,theta):
	z = np.dot(x,theta)
	return -y*np.log(sigmoid(z)) 
	- (1-y)*np.log(1 - sigmoid(z))

""" 
定义代价函数评估
	:param x:输入矩阵
	:param y:分类向量
	:param theta:拟合参数theta
	:return:预测精度
"""
def J(x,y,theta):
	return np.mean( cost(x,y,theta) )

"""
定义逻辑回归训练函数
	:param train_set_x:训练集--特征
	:param train_set_y:训练集--分类
	:param theta:初始化参数theta
	:param options:训练选项:
				   1.alpha 学习率
				   2.max_loop 最大迭代次数
				   3.eplison 收敛精度
				   4.debug 是否观察预测精度J的变化
				   5.method 训练方法:BGD？SGD？
	:return :拟合参数
"""

def trainLogicRegression(train_set_x,train_set_y,theta,options):
	# m:样本数
	# n:特征数
	m,n = train_set_x.shape
	
	# 迭代次数
	count = 0

	# 训练参数初始化
	alpha = options['alpha']
	max_loop = options['max_loop']
	eplison = options['eplison']
	debug = options['debug']
	method = options['method']

	methods = {
		'BGDdebug':BGDdebug,
		'SGDdebug':SGDdebug,
		'BGD':BGD,
		'SGD':SGD,
	}

	# 保存上次参数theta
	last = theta.copy()
	if debug:
		theta,count= methods.get(method+'debug')(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m)
	else:
		theta,count= methods.get(method)(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m)
	print "The number of iteration is :%d"%count
	return theta,count

"""
定义Batch Gradient Descent
"""
def BGDdebug(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m):
	plt.figure()
	while count <= max_loop:
		count=count+1
		diff = sigmoid(np.dot(train_set_x,theta)) - train_set_y
		theta = theta - alpha*np.dot(train_set_x.T,diff)/m
		# 绘制预测精度状况
		plt.scatter(
			count,
			J(train_set_x,train_set_y,theta),
			marker='o',
			color='y',
			s=50)

		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	plt.show()
	return theta,count

def BGD(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m):
	while count <= max_loop:
		count=count+1
		diff = sigmoid(np.dot(train_set_x,theta)) - train_set_y
		theta = theta - alpha*np.dot(train_set_x.T,diff)/m
		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	return theta,count

"""
定义Stotochastic Gradient Descent
"""
def SGDdebug(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m):
	plt.figure()
	while count <= max_loop:
		count=count+1
		for i in range(m):
			diff = sigmoid(np.dot(train_set_x[i],theta)) - train_set_y[i]
			theta = (theta.T - alpha*(train_set_x[i])*diff).T
		# 绘制预测精度状况
		plt.scatter(
			count,
			J(train_set_x,train_set_y,theta),
			marker='o',
			color='y',
			s=50)

		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	plt.xlabel('Iteration Step')
	plt.ylabel('Cost Evaluation')
	plt.show()
	return theta,count

def SGD(train_set_x,train_set_y,theta,alpha,eplison,max_loop,count,last,m):
	while count <= max_loop:
		count=count+1
		for i in range(m):
			diff = sigmoid(np.dot(train_set_x[i],theta)) - train_set_y[i]
			theta = (theta.T - alpha*(train_set_x[i])*diff).T
		if np.max(abs(theta-last)) < eplison:
			break
		else:
			last = theta.copy()
	return theta,count

"""
定义结果绘制函数
	:param train_set_x:训练集--特征
	:param train_set_y:训练集--分类
	:param theta:参数向量
"""
def showTrainResult(train_set_x,train_set_y,theta):
	# m:样本数
	# n:特征数
	m,n = train_set_x.shape

	z = np.arange(np.min(train_set_x[0:m,1:2]),np.max(train_set_x[0:m,1:2]),2)
	w = (-theta[0,0] - theta[1,0] * z) / theta[2,0]

	plt.figure()
	# 绘制样本点
	for i in range(m):
		if train_set_y[i]==0:
			plt.scatter(train_set_x[i,1],train_set_x[i,2],marker='o',color='y',s=50)
		else:
			plt.scatter(train_set_x[i,1],train_set_x[i,2],marker='*',color='r',s=50)
	plt.plot(z,w)
	plt.xlabel('X1')
	plt.ylabel('X2')
	plt.show()		
```

 
以下是测试函数，样本容量为100：

```python
# coding:utf-8
'''
    - Created with Sublime Text 2.
    - User: yoyoyohamapi
    - Date: 2015-01-09
    - Time: 14:39:03
    - Contact: yoyoyohamapi.com
'''
###################################
#                                 #
#      测试Logic Regression       #
#                                 #
###################################

import numpy as np
import time
from logic_regression import *

# 载入数据
b = np.loadtxt("data.txt")

# 特征集
train_set_x = np.ones([b.shape[0],b.shape[1]-1+1],dtype=float)

# m:样本数
# n:特征数
m,n = train_set_x.shape

# 初始化训练样本
train_set_x[0:m,1:2] = b[0:m,0:1]
train_set_x[0:m,2:3] = b[0:m,1:2]

# 分类向量
train_set_y = b[0:m,2:3]

# 初始化theta
theta = np.ones([n,1],dtype=float)

# 迭代次数
count = 0

# 定义训练参数
options = {
	'alpha':0.01,
	'max_loop':5000,
	'eplison':0.001,
	'debug':True,
	'method':'SGD'
}

#########################
#--------训练开始-------#
#########################

print 'Trainning start...................'
# 计时
start = time.clock()
# 训练 
theta,count = trainLogicRegression(train_set_x,train_set_y,theta,options)
# 停止计时
end = time.clock()
time_cost = end - start

#########################
#--------显示结果-------#
#########################
print 'Trainning end,the number of iteration is:%d'%count
print 'Time consumed:%ld'%time_cost+'s'
print 'Vector theta is:'
print theta
showTrainResult(train_set_x,train_set_y,theta)
$J(\theta)下降函数的图像如下$:
```

![jtheta](http://7pulhb.com1.z0.glb.clouddn.com/ml-COST.png)

分类结果如下：

![result](http://7pulhb.com1.z0.glb.clouddn.com/ml-CLASSIFICATION.png)

函数执行情况如下，分类100个样本，参数调节利用SGD算法，学习率为0.01，共迭代2206次，耗时17s左右，其实{% math \theta %}的初值设定将会极大的影响迭代次数，越好的初值，决策边界的转动（亦即做决定时的犹豫）就越少。

![function_result](http://7pulhb.com1.z0.glb.clouddn.com/ml-RESULT.png)