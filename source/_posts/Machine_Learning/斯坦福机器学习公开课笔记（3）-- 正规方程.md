title: 斯坦福机器学习公开课笔记（3）-- 正规方程
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----
###Normal Equation（正规方程）

前面论述的线性回归问题中，我们通过梯度下降法（Gradient Descent）来求得$J(\theta)$的最小值，但是对于学习率$\alpha$的调节，对于迭代任务的繁重，有时候使得我们非常恼火。为此，我们可通过如下矩阵式来获得$J(\theta)$的最小值（推导过程不赘述了，可以上Coursera上看）：

{% math_block %}
\theta=(X^TX)^{-1}X^Ty
{% endmath_block %}
其中，$X$为输入向量矩阵，特征零{% math x_0 %}记得置为1($x_0=1$）,$y$为目标向量,仅从该表达式形式上看，我们也脱离了学习率$\alpha$。

*****************************************
下面是Gradient Descent与Normal Equation的特点比较：

_Gradient Descent_:

1. 需要选择适当的学习率$\alpha$

2. 需要进行多步迭代

3. 对多特征适应性较好，能在特征数量很多时仍然工作良好

4. 能应用到一些更加复杂的算法中，如逻辑回归（Logic Regression）等

_Normal Equation_:

1. 不要学习率$\alpha$

2. 不需要进行迭代，在Matlab等平台上，矩阵式仅需一行代码就可完成

3. 需要计算矩阵式，其算法复杂度为$O(n^3)$，所以如果特征维度太高（特别是超过10000维），那么不宜再考虑该方法。

4. 对于一些更复杂的算法，该方法无法工作

下面是Normal Equaltion的测试代码，数据量不大，拟合结果非常优秀

```python
#coding:utf-8
import numpy as np
import math
import matplotlib.pyplot as plt


# 读取数据
b = np.loadtxt("ex1data1.txt")

# 定义样本数
m = b.shape[0]

# 初始化输入数据
x = np.ones([m,2],dtype=float)
x[0:m,1:2] = b[0:m,0:1]

# 目标向量
y = b[0:m,1:2]

# 定义Normal Equation
def normalEq(x,y):
	return np.dot(np.dot(np.matrix(np.dot(x.T,x)).I,x.T),y)

# 获得theta
theta = normalEq(x,y)

# 定义预测函数
def h(theta,x_i):
	return x_i*theta

# 定义代价函数
def J(theta,x,y,m):
	result = 0
	for i in range(m):
		diff = h(theta,x[i]) - y[i]
		result = result + pow(diff,2)
	return result/(2*m)

#显示结果

print "the cost is:"
print J(theta,x,y,m)
print "the theta is:"
print theta

z = np.linspace(0,np.max(x[0:m,1:2]),100)
w = theta[0,0] + theta[1,0]*z

plt.figure()
plt.scatter(x[0:m,1:2],y,10,c='y')
plt.plot(z,w)
plt.show()
```

![测试结果](http://7pulhb.com1.z0.glb.clouddn.com/ml-Normal_Equation_Demo.png)