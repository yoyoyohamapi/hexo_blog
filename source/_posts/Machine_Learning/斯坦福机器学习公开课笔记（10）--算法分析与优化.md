title: 斯坦福机器学习公开课笔记（10）-- 算法分析与优化
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----

## 陷入不归路的调试
OK,回顾一下我们在讨论线性回归时候所用的代价函数:

{% math_block %}
J(\theta) = \frac{1}{2m} [\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2+\lambda\sum\limits_{i=1}^m\theta_j^2)]
{% endmath_block %}
如何减小我们的预测代价(即提高我们的)，通常我们会这样思考：

1.  去采集更多的训练样本（见多识广会让人变得聪明，然而也会让人变得优柔寡断）
2.  降低特征维数以防止过拟合（无形中丧失了一部分可能非常有用的知识）
3.  获得更多的特征（增加了算法计算包袱，也会带来一些干扰项）
4.  添加一些多项式特征来获得更好的拟合（如{% math x_1^2 %},{% math x_2^2 %},{% math x_1x_2 %}等等，但也容易带来过拟合问题）
5.  尝试减小或增大 {% math \lambda %} 的值（纯粹凭感觉）

以上列出的清单除了最后一点，其余都会让你花费大量时间去尝试，而且这种尝试往往是凭借直觉做出的，并且如上面括号内的内容所示，这些尝试又有这些尝试的副作用，因而，你的努力最后往往只能是--“然而并没有卵用”。

## 诊断（Diagnostic）
为了避免陷入凭借直觉调试算法的不归路，我们应当耐心的、静下心来看看我们的算法到底出了什么问题，这就是在对我们的算法进行“诊断”，然后根据“病因”，对症下药。

### 评估你的假设函数（hypothesis）

在之前的章节中，我们已经知道仅仅具备一个很小的训练误差并不能保证我们的假设函数就是优秀的，因为这种“优秀”仅仅体现在了对于已知的训练样本的假设上，而无法保证见到新的样本时候还能做出足够好的预测，过拟合就是当中的典型例子。

![过拟合](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_过拟合.jpg)

为了科学地评估我们的假设函数，我们就应当去评估假设函数的泛化（generalize）能力，即对新样本的预测能力，为此，我们就考虑将数据集（Dataset）分为两个部分：

* __训练集（Training Set）__：70%
* __ 测试集（Test Set）__：30%

那么就引入如下符号：

* 训练集中的一个样本：{% math (x^{(1)},y^{(1)}) %}
* 测试集中的一个样本：{% math (x_{test}^{(1)},y_{test}^{(1)}) %}
* 训练集样本容量：{% math m %}
* 测试集样本容量：{% math m_{test} %}

> !注意：有时候数据集是按照某种顺序排列的，在划分训练集和测试集之前，我们最好先对数据集乱序，或者随机抽取训练集、测试集。

那么，对于线性回归问题，我们有如下的训练\测试过程：

- 从训练集中通过最小化代价函数$J(\theta)$学习到参数{% math \theta %}

- 计算测试误差（test set error）

{% math_block %}
J_{test}(\theta)=\frac{1}{2m_{test}}\sum\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2
{% endmath_block %}

如果是逻辑回归，我们就这样计算测试误差

{% math_block %}
J_{test}(\theta)=-\frac{1}{m_{test}}\sum\limits_{i=1}^{m_{test}}y_{test}^{(i)}logh_\theta(x_{test}^{(i)})+(1-y_{test}^{(i)})logh_\theta(x_{test}^{(i)})
{% endmath_block %}

- 并引入__误分率（Misclassification Error）__:

{% math_block %}
err(h_\theta(x),y)=\begin{cases} 1 &\mbox{if $h_\theta(x) \geq 0.5,y=0 $ or $h_\theta(x) \lt 0.5,y=1$} \\ 0 &\mbox{otherwise} \end{cases}
{% endmath_block %}

* 测试误差就可以更直观的表示：

{% math_block %}
Test_{error} = \frac{1}{m_{test}}\sum\limits_{1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y_{test}^{(i)})
{% endmath_block %}

### 模型选择（Model Selection ）
在多项式回归中，我们总是斟酌假设函数的多项式的次数到什么程度为好，我们会把多项式的精确程度称之为degree，我们也将所斟酌出的假设函数称之为模型。

在下例中，我们设定了几个具有不同degree的假设函数 {% math h_\theta(x) %}，并且每个我们都计算出了能使预测代价最小的向量  {% math \theta^{(i)} %} ,以及对应的测试误差 {% math J_{test}(\theta^{(i)}) %} 。

![Degree例](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_degree.jpg)

显然，我们会选择预测误差最小的假设函数，本例中，假设我们选择了{% math degree=5 %} 的假设函数：

{% math_block %}
h_\theta(x) = \theta_0+\theta_1x+.....+\theta_5x^{5}
{% endmath_block %}

那么如果再用测试误差评价“我们的这次选择”就有失偏颇了，因为这次选择本身就是根据测试误差做出的，因此，为了评估我们对于模型的选择是否恰当，就要增加一个新的数据集----__交叉验证集（Cross Validation Set）__。

通常，训练集，交叉验证集，测试集的占总数据集的比例为：6:2:2。

我们分别定义三个集合对应的误差函数：

* __测试集误差__： 

{% math_block %}
J_{train}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y{(i)})^2
{% endmath_block %}

* __交叉验证集误差__:

{% math_block %}
J_{cv}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x_{cv}^{(i)})-y_{cv}{(i)})^2
{% endmath_block %}

* __测试集误差__：

{% math_block %}
J_{test}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x_{test}^{(i)})-y_{test}{(i)})^2
{% endmath_block %}

现在，我们做出选择的依据是通过判断交叉验证集误差--- {% math J_{cv}(\theta) %} 的大小，”交叉“二字”承上启下“的作用也在此得以体现， 他指导我们做出模型选择，同时又为测试集的”发光发热“留出一条路，让测试集能够评价我们的选择是否恰当。

![根据交叉验证集误差选择假设函数](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_cv_degree.jpg)

### 偏差（bias）问题还是方差（variance）问题？

有时候，我们需要弄清__欠拟合__与__过拟合__问题是与bias有关？还是与variance有关？还是与二者都有关。

先明确两个概念：

__High bias__： 高偏差，太多数据游离在了拟合曲线之外，造成__欠拟合__。

__High variance__：高方差，曲线变化较大，如山路十八弯一般，造成__过拟合__。

![bias & variance ](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_bias_variance.jpg)

下面我们绘出 {% math J_{cv}(\theta) %} 与 {% math J_{train}(\theta) %} 随 {% math degree %} 变化的曲线：

![bias & variance plot](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_bias_variance_plot.jpg)

如图所示，在 {% math d=1 %} 处，{% math J_{cv}(\theta) %} 与 {% math J_{train}(\theta) %} 的误差都很高，亦即拟合状况非常差，出现了高偏差（欠拟合）的情况。

而在 {% math d=4 %} 处， {% math J_{train}(\theta) %} 的误差很低，说明该假设函数曲线拟合不错，然而{% math J_{cv}(\theta) %} 却很高，又表明该假设函数泛化能力很低，综上，在 {% math d=4 %} 处，出现了__过拟合__状况。借助该图，我们能够定位到 {% math J_{train}(\theta) %} 及 {% math J_{cv}(\theta) %} 都很低的位置，据此，我们就能得到最佳的 {% math degree %} 取值。

### 正则化（Regularization）与偏差（bias）及方差（variance）的关系

在之前的章节中我们学习到，正则化（{% math \lambda %}）够帮助我们解决过拟合和欠拟合问题，现在，我们着手研究正则化与偏差及方差间的关系。

通过下例，我们简单回顾下正则化：

![Regularization](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_lambda.jpg)

通过之前的学习，我们知道，{% math \lambda %} 取值越大，对 {% math \theta %} 的惩罚力度就越大，反之越小，例如上图中，我们取值 {% math \lambda=10000 %} 时，{% math \theta_1,\theta_2... %} 都被惩罚的近似于 {% math 0 %} ,此时有：

{% math_block %}
h_\theta(x) \approx \theta_0
{% endmath_block %}

即假设函数是一条平直的直线，显然这样就出现了欠拟合（高偏差）现象。而若我们取值 {% math \lambda=0 %} 时，也就相当于没做正则化，此时就会出现过拟合（高方差）状况。

下例中，我们通过评估交叉验证集误差 {% math J_{cv}(\theta^{(5)}) %} 来选择合适的 {% math \lambda %} ,假定我们选择了 {% math \theta^{(5)} %} 对应的假设函数 ，然后我们通过测试误差 {% math J_{test}(\theta^{(5)}) %} 来评估我们的选择是否恰当。

![](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_picking_lambda.jpg)

下面我们绘出 {% math J_{cv}(\theta) %} 与 {% math J_{train}(\theta) %} 随 {% math lambda %} 变化的曲线：

![变化曲线](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_lambda_cvtrain_plot.jpg)

通过上图曲线，能帮我们找到最佳的正则化 {% math \lambda %} 取值（图中just right处的取值），而避免出现高偏差和高方差的状况。


### 训练样本数（m）与偏差及方差的关系
在上几节中，我们学会了如何选择正确的 {% math degree %} 进行多项式回归，以及选择恰当的 {% math \lambda %} 进行正则化，下面我们要讨论是否增加训练样本数 {% math m %} 能够提升算法性能。

先看下图中的__学习曲线（learning curve）__，当样本数很低时，我们的训练误差 {% math J_{train}(\theta) %}是很低的,因为此时拟合难度非常低，当然，样本的不足也就意味着假设函数的泛化能力较低，故而交叉验证集的误差 {% math J_{cv}(\theta) %} 非常大。随着训练样本的增多，假设函数的泛化能力增强（知识储备变多），而拟合难度变大，故而 {% math J_{train}(\theta) %} 升高，而 {% math J_{cv}(\theta) %} 降低。

![学习曲线](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_m_error.jpg)
 
那么如果我们已经遇到了高偏差（欠拟合）时，学习曲线会是怎样的呢？

![高偏差下的学习曲线](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_m_high_bias.jpg)

如上图所示，显然，当我们遇到高偏差（欠拟合）的状况时，再增大训练样本的容量都是徒劳的，因为此时 {% math J_cv %} 与 {% math J_train %} 都很高，且不随 $m$ 的增大而降低。

再看高方差（过拟合）下的学习曲线：

![高方差下的学习曲线](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_m_highvariance.jpg)

显然，在遇到高方差（过拟合）的状况时，我们非常有必要增大训练样本的容量来提高假设函数的泛化能力。

## 构建一个垃圾邮件分类器
接下来，通过构建一个垃圾邮件分类器来继续探讨如何进行机器学习的算法优化。

我们令向量 {% math x %} 表示垃圾邮件的特征，而令 {% math y %} 表示该邮件是否是垃圾邮件：

{% math_block %}
y=\begin{cases}1 &\mbox{是垃圾邮件} \\ 0 &\mbox{不是} \end{cases}
{% endmath_block %}

我们可以选取100个单词作为邮件特征，且单词按字母排序。并令：

{% math_block %}
x_j=\begin{cases} 1 &\mbox{如果单词$j$出现} \\ 1 &\mbox{未出现} \end{cases}
{% endmath_block %}

![spam feature](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_spam_classifier.jpg)

> !注意：在实际中，手动随机选择100个单词的体量是远远不够的，应当选择最常出现的10000~50000个单词。

那么如何降低分类的错误率？我们不难想出如下四种办法：

*  尽可能多地收集数据
	[Honeypot-蜜罐](http://www.honeynet.org/) 就是做这样一件事，通过把自己包装成一个对于黑客极具吸引力的机器，来吸引黑客攻击，收集攻击数据。

* 建立以Email路由为信息的特征

* 根据邮件正文建立特征
	- 是否discount与discounts可以被一概视之
	- 是否需要考虑标点符号（诸如购物广告这样的垃圾邮件很可能充斥着诱人的感叹号）

* 垃圾邮件制造者不是白痴，为了防止他们的邮件被检测为垃圾邮件，这些攻击者往往会在一些单词拼写上做手脚，例如：m0rtgage，med1cine，w4tches等等。所以考虑设计算法来检查这些错误拼写。

### 进行错误分析（Error Analysis）
例：有500个验证样本，即$m_{cv}=500$，算法错分了100封邮件，下面我们进行错误分析，也就是手动检查者错分的100封邮件，并且：

（1） 将其按类型分类（促销邮件，钓鱼邮件等等）
（2） 据此总结出一些线索或者特征帮助提高分类准确性（比如观察到了钓鱼邮件最多，我们就尝试手动地找出一些钓鱼邮件共有的特征）

这也就是为什么吴恩达教授钟情于先快速编写一个基本而粗糙的算法，这样有助于今早的发现，归纳和改正错误。


### 进行数值评估（Numerical Evaluation）
这部分很好理解，就是我们有时候应当量化一些策略，来评估该策略的采用是否显著提高了学习算法的性能表现。比如通过比较采用英语分词算法（stemming）前后 {% math J_{cv}(\theta) %} 的大小就可以直接看出是否该算法能够帮助我们提高学习能力。

### 偏斜类（Skewed Classes）,准确率（Precision）与召回率（Recall）
例：有如下的__逻辑回归__模型  ：

{% math_block %}
\begin{cases} y=1 &\mbox{病人患病} \\ y=0 &\mbox{病人未患病} \end{cases}
{% endmath_block %}

且令人欣喜的是，测试集中只出现了 __1%__ 的误诊,但是存在这样的一种情况：测试样本中只有 __0.5%__的人群患癌（这就是所谓的偏斜类，即颇为怪异的情况），那么我们令预测函数为：

{% math_block %}
h_\theta(x)=0
{% endmath_block %}

亦即我们总预测病人没有患癌，那么我们预测错误率将会更低，然而这种预测显得极为滑稽。所以单纯的预测错误率并不能真实的反映的我们假设函数的好与坏，我们需要引入其他评价指标：__准确率（Precision）__，__召回率（Recall）__。为此，我们还要定义__阳性（Positive）__与__阴性（Negative）__，以及评估他们的__真（True）__、__伪（False）__：

* 阳性：表示真值，阳性为真（True Positive）则代表预测与实际都为真，即预测正确；同理可知假阳性（False Positive）为预测为真，实际为伪，预测错误。

* 阴性：表示伪值，真阴性（True Negative）表示预测与实际皆为伪，预测正确；假阴性（False Negtive）表示预测为伪，而实际为真，预测错误。

> 真假指代是否预测正确，阴阳显示作出了什么预测。

由此可得Precision与Recall的定义式：

{% math_block %}
Precision=\frac{True Positive}{Predicated Positive}=\frac{True Pos}{True Pos + False Pos}
{% endmath_block %}

（预测病人患病有多少预测对了---》预测准确率）

{% math_block %}
Recall=\frac{True Positive}{ActualPositive}=\frac{TruePos}{TruePos+FalsePos}
{% endmath_block %}

（在真实患病的人中，有多少被成功预测到了--》预测的覆盖率）

理想状况下，我们希望假设函数能够同时具备高准确率（High Precision）及高召回率（High Recall）。但是往往鱼和熊掌不可兼得，看下面这个例子：

一开始，我们有在假设函数

{% math_block %}
h_\theta(x) \geq 0.5
{% endmath_block %}

时预测病人患癌，但这样风险还是比较大，我们想在有充足自信认为病人患癌是才预测病人患癌，因此我们提高了预测阈值:

{% math_block %}
h_\theta(x) \geq 0.7
{% endmath_block %}

现在，我们的预测更加准确，即High Precision，然而，对于部分体征不明显但确实患癌的病人却被我们认为是不患癌，故而，真实患病者中被确诊患病的比例就有所降低（Low Recall）。

在这个例子中，一旦我们提高了precision，recall就会随之降低，这也反映出二者实际构成了一组矛盾，因而单纯通过其中任何一个去评价算法好坏都是有失偏颇的。为此，我们需要一个综合了两者的关系式去评估算法---{% math F_1Score %}:

{% math_block %}
F_1Score = \frac{PR}{P+R} , \mbox{ P is presicion ,R is recall }
{% endmath_block %}

从这个定义式中我们可以看到，{% math F_1Score %} 给了较小值一个很大的比重，如果小值很小，那么{% math F_1Score %} 也很小，且有：

{% math_block %}
\begin{cases} P=0 &\mbox{or}& R=0 , F_1=0 \\ P=0 &\mbox{and}& R=0 , F_1=1\end{cases}
{% endmath_block %}

![F1SCORE](http://7pulhb.com1.z0.glb.clouddn.com/ML-w6_f1score.jpg)


>　大部分时候，{% math F_1Score %}足够用了。

### 使用大量数据

这里不展开细讲了，仅需记住一句经典的话：

> "It's not who has the best algorithm that wins.It's who has the most data."

没有最好的算法，只有更多的数据。
