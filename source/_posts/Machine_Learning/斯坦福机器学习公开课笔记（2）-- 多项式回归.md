title: 斯坦福机器学习公开课笔记（2）-- 多项式回归
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----
###通过Polynomial Regression（多项式回归）来获得更好的拟合：

在一开始的线性回归中，我们通过如下算式来对房价进行预测，其中$size$表示房屋面积:

{% math_block %}
h_\theta(x)=\theta_0+\theta_1*size
{% endmath_block %}

但是可能这样拟合出的曲线较为粗糙，那么我们可考虑对$size$进行平方，即获得更加精准的$size$变化情况，将其平方值当做特征之一进行预测：

{% math_block %}
h_\theta(x)=\theta_0+\theta_1*size+\theta_2*size^2
{% endmath_block %}

这样会带来更好的拟合曲线，但是形成的二次曲线有个问题，就是在房屋面积足够大时，房价会随着房屋面积的增大而降低，这明显不符合客观规律:

![error](http://7pulhb.com1.z0.glb.clouddn.com/ml-PolynomialRegr.png)

所以我们可以考虑将二次方换为开方或者在加上三次方项ß，得到如下两种预测：

（1） 
{% math_block %}
h_\theta(x)=\theta_0+\theta_1*size+\theta_2*size^2+\theta_3*size^3
{% endmath_block %}

（2）

{% math_block %} 
h_\theta(x)=\theta_0+\theta_1*size+\theta_2*\sqrt{size}
{% endmath_block %}

考虑到三次方项会带来很大的值，所以更偏向于选择第（2）种预测。

注意，多项式回归中因为各特征数值上差异过大（例如 {% math size \in (1,10000),\sqrt{size} \in (1,100) %}），为使各特征在预测过程中被同等看待，就要进行feature scaling（特征缩放），将各特征都缩放到$[0,1]$，即用__比重__来衡量特征，而非数值。考虑下面这个例子：

假定房屋面积对房价的预测模型如下：

{% math_block %} 
h_\theta(x)=\theta_0+\theta_1*size+\theta_2*\sqrt{size}
{% endmath_block %}

并且，房屋面积在1至1000内变动，拟合模型为：

{% math_block %} 
h_\theta(x)=\theta_0+\theta_1*x_1+\theta_2*x_2
{% endmath_block %}

那么，需要完成如下feature scaling($\sqrt{1000}\approx{32}$):

{% math_block %} 
x_1=\frac{x_1}{1000},x_2=\frac{x_2}{32}
{% endmath_block %}

以下是Polynomial Regression问题的测试代码：

```python
#coding:utf-8
import numpy as np
import math
import matplotlib.pyplot as plt

# 读取数据
b = np.loadtxt("ex1data1.txt")

# 初始化输入数据
x = np.ones([b.shape[0],3],dtype=float) 
x[0:x.shape[0],1:2] = b[0:b.shape[0],0:1]
x[0:x.shape[0],2:3] = np.sqrt(b[0:b.shape[0],0:1])

# 目标数据
y = b[0:b.shape[0],1:2]

# 定义theta向量
theta = np.matrix([[0.0],[0.0],[0.0]])

# 样本数m
m = x.shape[0]

# 特征数
n = x.shape[1]

# 特征1的最大值
x_1_max = np.max(x[0:m,1:2])

# 特征2的最大值
x_2_max = np.max(x[0:m,2:3])

#是否进行feature scaling
# fs_flag = False
fs_flag = True


# 定义最大迭代次数
max_loop = 10000

# 定义收敛精度
eplison = 0.0001

# 定义学习率
rate = 0.01

# 定义预测函数
def h(theta,x_i):
	return x_i*theta

# 定义代价函数
def J(theta,rate,x,y,m):
	result = 0
	for i in range(m):
		diff = h(theta,x[i]) - y[i]
		result = result + pow(diff,2)
	return result/(2*m)

# 定义随机梯度下降函数
def sgd(theta,x,y,m,n):
	count = 0
	error = np.matrix([[0.0],[0.0],[0.0]])
	while count <= max_loop:
		count = count+1
		for i in range(m):
			diff = y[i] - h(theta,x[i])
			for j in range(n):
				theta[j] = theta[j] + rate*diff*x[i,j]
		if( np.max(abs(theta-error)) < eplison ):
			break;
		else:
			error = theta.copy()
	print "Iteration has been exuecuted for:%d"%count+" times"

# *******************训练开始******************************

# 保存x方便显示
x_src = x.copy()

# 标定
if fs_flag == True :
	for i in range(m):
		x[i,1] = x[i,1]/x_1_max
		x[i,2] = x[i,2]/x_2_max

sgd(theta,rate,x,y,m,n)

# 显示结果
print theta

z = np.linspace(0,x_1_max,100)
w = theta[0,0] + theta[1,0]*z + theta[2,0]*z**0.5

plt.figure()
plt.scatter(x_src[0:m,1:2],y,10,c='y')
plt.plot(z,w)
plt.show()
```

学习过程总过迭代了2555次，学习率为0.01，特征变动不大，故未采用feature scaling（而且貌似我用了feature scaling反而学习不出来了）：

![测试结果](http://7pulhb.com1.z0.glb.clouddn.com/ml-polynomialDemo.png)