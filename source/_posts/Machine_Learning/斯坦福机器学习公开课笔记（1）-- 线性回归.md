title: 斯坦福机器学习公开课笔记（1）-- 线性回归
tags: 斯坦福机器学习
categories: 斯坦福机器学习公开课笔记
----
##线性回归(Linear Regression)问题

假定我们有一大批房屋的面积以及该面积下房屋价格的数据，我们想要据此得到房屋面积与房屋价格间的关系（回归（regression）问题：从某种输入得到与之关联的输出），那么，对于新出现的房屋面积，我们就能合理地预测其价格了。

##解决方案

上面的问题其实就被刻画成了
> “OK，我具备了很多关于房屋面积及其对应售价的知识（数据），那么再通过一定的学习，当面对新的房屋面积时，我不再对其定价感到束手无策”。

据此不难得到问题的解决思路：

1. __积累知识__: 我们将知识称之为_Training Set_，即_训练集_，这很好理解,知识能够训练人进步

2. __学习__: 学习阶段，应当有合适的指导方针，江山不是土匪能打下来的，这里合适的指导方针我们称之为_Learning Algorithm_，学习的产出是教会了我们如何预测。

3. __预测__: 学习完成，给定你一个房屋面积，要能做出预测

但这并非一个直线型的学习过程，因为知识会不断膨胀，并且“人谁无过，过而能改，善莫大焉”，学习是一个无止境的过程，一旦预测错误，就势必应当改正自己的学习的学习策略。
***************************************************

## 基础知识

首先，明确几个常用的数学符号：

* 特征（feature）：$x_i$， 比如房屋的面积,卧室数量都是特征

* 特征向量（输入）：$x$，则${x_j}^(i)$表示第i个输入向量的第$j$个特征

* 类型（输出）：$y_i$，如某个房屋的售价即是我们根据房屋面积，卧室数量组成的特征向量推导出的输出。

* 假设（hypothesis）：假设即给定特征向量，我们如何做出预测，也就可以把假设看作是关于$x$的预测函数，例如下面的假设就是一个线性假设

{% math_block %}
h_\theta(x)=\theta_o+\theta_1x_1+\theta_2x_2=\theta^Tx
{% endmath_block %}

由于 {% math x %} 是特征，其在输入后即固定不变，那么向量 $\theta$ 就成了保证我们预测准度的基石所在，但是预测终究只是预测，其与真实值 $y\_i$ 之间就必定存在误差，这个误差反过来也可以作为一种评价手段，帮助我们调教 $\theta$ 值，使得我们的预测更加精准。
例如，我们通过_最小均方（Least Mean Square）_来衡量误差：

{% math_block %}
J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m[ h_\theta(x^{(i)})-y^{(i)} ]^2
{% endmath_block %}
其中，$m$为样本数

这种评价手段在机器学习中称为——_cost function_，_代价函数_，即我们做出某种预测相比较于真实值所要付出的代价。
*****************************************************
OK，那么我们现在目的很明确了，就是反复调节 $\theta$ 使得 $J(\theta)$ 足够小，即预测所付出的代价足够小，亦即我们的预测精度越高。
下面引入一种叫_梯度下降(Gradient Descent)_的调节手段:

{% math_block %}
\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
{% endmath_block %}

其中，$\alpha$为学习率（learning rate）

数学上，梯度方向是函数值下降最为剧烈的方向，显然沿着$J(\theta)$的梯度方向走，我们就能接近其最小值，或者极小值，亦即接近最高预测精度，而_学习率_是相当玄乎的一个参数，它标识了沿梯度方向行进的速率，步子大了容易扯着蛋，否则我们可能在下一步时错过了最小值，又要掉头往回走。
所以在实际编程中，学习率的取值可以以3倍，10倍尝试取值进行测试，如：

0.001,0.003,0.01........0.3,1

易推导：

{% math_block %}
\frac{\partial}{\partial\theta_j}J(\theta)=(h_\theta(x)-y)x_j^{(i)}
{% endmath_block %}

据此，对于一个训练集，我们定义参数的调节规则为：

Repeat Until Convergence{
{% math_block %}
\theta_j=\theta_j+\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
{% endmath_block %}
}

其中，代价函数为：

{% math_block %}
J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m[ h_\theta(x^{(i)})-y^{(i)} ]^2
{% endmath_block %}

我们称这条规则__基于最小均方（LMS）的批量梯度下降法（Batch Gradient Descent）__，该方法虽然可以收敛（即总能到达极小值），但是每更新一个参数，我们都不得不过一遍样本集（ $m$ ），如果样本集体积很大，这显然开销巨大。

为此，定义__随机梯度下降（Stochastic Gradient Descent）__规则：

Loop Until Convergence
for i=1 to m {
{% math_block %}

对于每个特征\theta_j: \\

\theta_j = \theta_j+\alpha(y{(i)}-h_\theta(x^{(i)}))x_j(i)
{% endmath_block %}
}

在批量梯度下降法，每次迭代过程中，更新某个参数必须遍历整个样本集，而在随机梯度下降法中，每次更新参数只需要一个样本集 {% math (x^{(i)},y^{(i)}) %}，通常，样本集容量巨大时，我们可能很早就能获得最优解，所以SGD优势明显。

总结一下：

* __批量梯度下降__：尽可能减小所有训练样本的预测代价，能够获得最小值，即最高的预测精度。

* __随机梯度下降__：尽可能的减小每个训练样本的预测代价，虽然并非从全局着眼，但是这种每个样本的最小化过程通常也能使得最终的预测精度足够高。
***************************************************************

## 程序范例

下述代码片利用随机梯度下降法完成了线性回归问题，测试数据来自公开课的练习一：

```python
import os
#coding:utf-8
import numpy as np
import matplotlib.pyplot as plt

# 读取数据
b = np.loadtxt("ex1data1.txt")

# 初始化输入数据
x = np.ones((b.shape[0],2),dtype=float)
x[0:x.shape[0],1:2] = b[0:b.shape[0],0:1]

# 目标数据
y = b[0:b.shape[0],1:2]

# 定义theta向量
theta = np.matrix([[0.0],[0.0]])

# 样本数m
m = x.shape[0]

# 特征数
n = x.shape[1]

# 特征的最大值
x_max = np.max(x[0:m,1:2])


# 定义最大迭代次数
max_loop = 10000

# 定义收敛精度
eplison = 0.001

# 定义学习率
rate = 0.01

# 定义预测函数
def h(theta,x_i):
	return x_i*theta

# 定义代价函数
def J(theta,rate,x,y,m):
	result = 0
	for i in range(m):
		diff = h(theta,x[i]) - y[i]
		result = result + pow(diff,2)
	return result/(2*m)

# 定义随机梯度下降函数
def sgd(theta,rate,x,y,m,n):
	count = 0
	error = np.matrix([[0.0],[0.0]])
	while count <= max_loop:
		count = count+1
		for i in range(m):
			diff = y[i] - h(theta,x[i])
			for j in range(n):
				theta[j] = theta[j] + rate*diff*x[i,j]
		if( np.max(abs(theta-error)) < eplison ):
			break;
		else:
			error = theta.copy()
	print "Iteration has been exuecuted for:%d"%count+" times"



# *******************训练开始******************************
# 保存x方便显示
x_src = x.copy()
sgd(theta,rate,x,y,m,n)

# 显示结果
print theta
z = np.linspace(0,x_max,1000)
w = theta[0,0]+theta[1,0]*z
plt.figure()
plt.scatter(x_src[0:m,1:2],y,10,c='y')
plt.plot(z,w)
plt.show()
```

拟合结果如图,迭代过程进行了29次，学习率为0.01：

![拟合结果](http://7pulhb.com1.z0.glb.clouddn.com/ml-linearDemo.png)

显然，直线能够大致拟合出房屋面积与房价的关系，但从上图也可以看到，这种拟合是相当粗糙的，很多数据点距离拟合直线太远，在下一节中，我们将采用曲线来进行拟合，以使得我们的预测更加准确。
